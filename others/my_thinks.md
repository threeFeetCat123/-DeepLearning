以下内容建议有一定基础后，再阅读，是个人对深度学习的一些见解，帮助你从系统上有一定的理解。内容并没有追求数学定义的严谨，追求概念上的串通。

个人粗暴地理解：

目前的深度学习基本都建立在 **反向传播** 的 **梯度下降** 的基本训练方法上。各种深度学习算法不同之处在于，他们各自不同的参数的框架。

### MP

**MP** 类似于只有一层转换

    输出 * 权重矩阵 = 输出（这里简单化了）。


### BP

**BP** 在 MP 的基础上，引入了以 **链式求导** 为准则的反向传播，实现了多层转换。

    输入 * 输入权重矩阵 = 中间值1

    中间值1 * 权重矩阵1 = 中间值2

    .......

    中间值n-1 * 权重矩阵n-1 = 中间值n

    中间值n * 输出权重 = 输出 

### RNN

**RNN** 关注于是时间上的循环，不是结构的循环。

**RNN** 更像是在 BP 的基础上引入了之前状态的影响，即前向传播的时候，不单单只受当前输入的影响（MP/BP都是由当前输入的决定输出），还有之前的状态决定。 

<center>
    <img src="run_rnn.gif" alt="RNN运行概况">
</center>

举个例子，输入层每次只能输入一个数字，第一次运行输入0，第二次运行输入1。

使用 BP/MP 时， 在第二次只能看到输入的1

但在RNN视角中，它还能看见第一次输入0的储存的中间值。当然第一次运行时，我们要把中间状态存储下来。